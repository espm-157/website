{"version":"1","records":[{"hierarchy":{"lvl1":"ESPM-157: Data Science in Global Change Ecology"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"ESPM-157: Data Science in Global Change Ecology"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"ESPM-157: Data Science in Global Change Ecology","lvl2":"ESPM-157: Data Science in Global Change Ecology"},"type":"lvl2","url":"/#espm-157-data-science-in-global-change-ecology","position":2},{"hierarchy":{"lvl1":"ESPM-157: Data Science in Global Change Ecology","lvl2":"ESPM-157: Data Science in Global Change Ecology"},"content":"Instructor Team\n\n👤 \n\nCarl Boettiger  \n\nOH: 9a Friday by appt👤 Amanda Wik, TA.👤 Tyler Bernard, TA. \n\nOH: 11:30a Fridays\n\nLogistics\n\n🏛 Wheeler 212📅 Mon/Weds noon-2pm\n\nESPM-157 seeks to introduce students to cutting-edge skills in data science and computer coding, including GitHub,\nreproducible notebooks, databases, data visualization, geospatial analysis and more in an inclusive and hands-on\ncollaborative setting. See our \n\nsyllabus and \n\nschedule overview\nfor details. Please see \n\nenrollment if you questions about getting into this course!","type":"content","url":"/#espm-157-data-science-in-global-change-ecology","position":3},{"hierarchy":{"lvl1":"🧰 Module Overview"},"type":"lvl1","url":"/schedule","position":0},{"hierarchy":{"lvl1":"🧰 Module Overview"},"content":"🌡 Module 1: Global Climate Change\n\n🖺 \n\nMann et al (1998)🛠 Concepts: Tabular data, gitCommunication: declarative visualization, jupyter notebooks\n\nclimate template\n\n🐟 Module 2: Global Fisheries Collapse\n\n🖺 \n\nWorm et al (2006)🛠 Concepts: Relational databases📈 Communication: Myst, pdf/LaTeX, html outputs\n\nfish template\n\n🛰  Module 3: Environmental Justice\n\n🖺 \n\nSchell et al (2020)🛠 Concepts: Geospatial data structures + analysis📈 Communication: Geospatial visualziation\n\nspatial template\n\n🤖 Module 4: Language Models and Decision support tools?\n\n🖺 TBD🛠 LLMs, RAG,📈 streamlit, reactive design\n\nllm template","type":"content","url":"/schedule","position":1},{"hierarchy":{"lvl1":"🧰 Module Overview","lvl2":"📅 Weekly Plan"},"type":"lvl2","url":"/schedule#id-weekly-plan","position":2},{"hierarchy":{"lvl1":"🧰 Module Overview","lvl2":"📅 Weekly Plan"},"content":"Hold on tight and don’t put too much faith in this.\n\nWeek\n\nModule\n\nTopics\n\nReading links\n\nweek 1\n\n🌡\n\nGitHub, JupyterHub\n\n{📖\n\nweek 2\n\n🌡\n\nviz: seaborn\n\n{📖\n\nweek 3\n\n🌡\n\ndata: pandas / ibis\n\n{📖\n\nweek 4\n\n🌡\n\nCollab,  PRs, CI\n\n{📖\n\nweek 5\n\n🐟\n\nModule intro\n\n{📖\n\nweek 6\n\n🐟\n\nRDBs, joins\n\n{📖\n\nweek 7\n\n🐟\n\ndiscovery\n\n{📖\n\nweek 8\n\n🛰\n\nspatial data types\n\n{📖\n\nweek 9\n\n🛰\n\nspatial data viz\n\n{📖\n\nweek 10\n\n🛰\n\nspatial data analysis\n\n{📖\n\nweek 11\n\n🛰\n\nputting it together\n\n{📖\n\nweek 12\n\n🤖\n\nLLMs\n\n{📖\n\nweek 13\n\n🤖\n\nLLMs+RAG?\n\n{📖\n\nweek 14\n\n🤖\n\nstreamlit\n\n{📖\n\nweek 15\n\n🎓\n\nFinal Projects\n\n{📖","type":"content","url":"/schedule#id-weekly-plan","position":3},{"hierarchy":{"lvl1":"⚖ Syllabus"},"type":"lvl1","url":"/syllabus","position":0},{"hierarchy":{"lvl1":"⚖ Syllabus"},"content":"ESPM-157: Data Science in Global Change Ecology\n\nFall 2024 - the existing ESPM-157 moves to Wheeler 212, more than doubles in size, and switches into Python! Much has changed, much will change, and this course is changing with it.  Welcome aboard.","type":"content","url":"/syllabus","position":1},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Textbook"},"type":"lvl2","url":"/syllabus#textbook","position":2},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Textbook"},"content":"There is no required textbook for this course at this time.  Recommended readings will be indicated when available and further developed during this course.","type":"content","url":"/syllabus#textbook","position":3},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Enrollment"},"type":"lvl2","url":"/syllabus#enrollment","position":4},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Enrollment"},"content":"Like too many UC Berkeley courses, ESPM-157 is oversubscribed. Registration for ESPM-157 follows the campus staged process with reserved seats for several of the specific majors in the host department (ESPM) as determined by Rasser College of Natural Resources (RCNR) scheduling and academic advising staff. All seats go quickly, so please plan to enroll as soon as you are eligible to do so. Students in ESPM-based majors should speak to their major adviser about enrolling in ESPM-157 and reserved seats.  Please do not contact course instructors about permission to enroll or enrollment codes for reserved seats, this process is handled by RCNR Scheduling to be as fair and equitable as possible across student groups needing this course to meet graduation requirements.  We are constantly exploring ways to expand access to this course without degrading the educational experience. Graduate students interested in 157 should consider \n\nESPM 288, usually offered each Spring.","type":"content","url":"/syllabus#enrollment","position":5},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Format and assessment"},"type":"lvl2","url":"/syllabus#format-and-assessment","position":6},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Format and assessment"},"content":"This course centers around our in-class experience. We meet for 2 hours twice a week for a hands-on, group-based experience facilitated by your instructors. Lectures will not be recorded because there will not be lectures. Built on decades of education research and delivered in the most recent state-of-the art Active Learning Classroom on campus, this course emphasizes a collaborative learning-by-doing approach in all elements. Today, there are a wealth of lectures in data science that can be viewed freely without a university tuition. Education research shows the value of hands-on, peer based experience which we have the opportunity to create in our classroom. We recognize that things happen and students will not be penalized if they cannot make a session\n\nThe course is built on four open-ended modules and final project.  In each, students will work in pairs (and occassionally individually) to replicate or refute the main scientific findings of a key paper or result in global change ecology.  Each module is selected to introduce a core set of competencies in data science, fundamental concepts in global change ecology, and new skills for reproducible research and data-driven communication.","type":"content","url":"/syllabus#format-and-assessment","position":7},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Prerequisites"},"type":"lvl2","url":"/syllabus#prerequisites","position":8},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Prerequisites"},"content":"This course has no formal prerequisites. Prior experience with programming in any language can be helpful. This course is uniquely structured with open-ended assignments that can be accessible to first time programmers or challenge experienced software developers.","type":"content","url":"/syllabus#prerequisites","position":9},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Office hours"},"type":"lvl2","url":"/syllabus#office-hours","position":10},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Office hours"},"content":"TBD","type":"content","url":"/syllabus#office-hours","position":11},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Communication"},"type":"lvl2","url":"/syllabus#communication","position":12},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Communication"},"content":"Guided by education research, our course design seeks to emulate the feel and experience at the cutting edge of Data Science research today. Technical communication platforms are a core component of this. Learning Management Systems (LMS) typically used in instructional settings do not provide an authentic experience. Students will be introduced to communication and collaboration tools including GitHub and Slack for most communication purposes.","type":"content","url":"/syllabus#communication","position":13},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Grading"},"type":"lvl2","url":"/syllabus#grading","position":14},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Grading"},"content":"Grades will be assigned using the following weighted components:\n\ncomponent\n\nweight\n\nFinal Project\n\n30%\n\nHW Exercises\n\n60%\n\nParticipation\n\n10%\n\nGrading rubrics will usually be provided with each module.\nIt is expected that students in this course will have a wide range of prior experience\nand ability, and grading will aim to reflect learning and effort in the course.\nThis course is not graded “on a curve” compared to other students.\nGrades will reflect relative individual learning and improvement from earlier benchmarks.\nIt is certainly possible for all students to receive high grades in this course\nif all of you show mastery of the material and completely attempt all assignments.\nHowever, high competency that does not reflect learning gains or the stylistic expectations of this\ncourse may score quite poorly. Our course prizes concise, semantic code supported by clear\nand insightful text and figures.\n\nAll assignments are due by immediately before the start of class on the day indicated. Assignments should be submitted as instructed.","type":"content","url":"/syllabus#grading","position":15},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Make-up policy"},"type":"lvl2","url":"/syllabus#make-up-policy","position":16},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Make-up policy"},"content":"Late assignments may be docked 20% and will not be accepted more than 48\nhours late except in cases of genuine emergencies\nor in cases where this has been discussed and approved in\nadvance. This policy is based on the idea that in order to learn how to\nuse computers well, students should be working with them at multiple times\neach week. Time has been allotted in class for working on assignments and\nstudents are expected to work on them outside of class. It is intended\nthat you should have finished as much of the assignment as you can based\non what we have covered in class by the following class period. Therefore,\neven if something unexpected happens at the last minute you should already\nbe close to done with the assignment. This policy also allows rapid\nfeedback to be provided to students by returning assignments quickly.","type":"content","url":"/syllabus#make-up-policy","position":17},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Code of Conduct"},"type":"lvl2","url":"/syllabus#code-of-conduct","position":18},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Code of Conduct"},"content":"Our course is committed to providing a respectful and welcoming\nenvironment to all participants.  Please review the\n\n\nOpen Code of Conduct\nguidelines for respectful and harassment-free conduct. To report\nan incident or request more information, contact the UC Berkeley\n\n\nOffice for the Prevention of Harassment and Discrimination by emailing ask_ophd@berkeley.edu or by phone (510) 643-7985.","type":"content","url":"/syllabus#code-of-conduct","position":19},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Learning Cooperatively"},"type":"lvl2","url":"/syllabus#learning-cooperatively","position":20},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Learning Cooperatively"},"content":"I encourage you to discuss all of the course activities with your friends\nand classmates and your instructional team as you are working on them.\nYou will definitely learn more\nin this class if you work with others than if you do not. Ask questions,\nanswer questions, and share ideas liberally. Please identify your\ncollaborators by name on all assignments.\n\nSince you’re working collaboratively, keep your project partner and the\ncourse instructor informed. If some medical or personal emergency takes\nyou away from the course for an extended period, or if you decide to drop\nthe course for any reason, please don’t just disappear silently! You\nshould inform your project partner, so that nobody is depending on you\nto do something you can’t finish.","type":"content","url":"/syllabus#learning-cooperatively","position":21},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Generative AI"},"type":"lvl2","url":"/syllabus#generative-ai","position":22},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Generative AI"},"content":"Generative AI is very much a part of the modern data science landscape and\nwill be a part of our course.  This technology also raises legal, ethical\nand environmental issues, as technology has done before. We will seek to\naddress and discuss both the opportunities and challenges through hands-on\nexperience with examples of this technology.\nThis course will explore not\nonly how generative AI can assist (and fail) in programming tasks, but how\nwe can run language models ourselves and build th into applications safely\nand reliably.\n","type":"content","url":"/syllabus#generative-ai","position":23},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Quiz/Exam Policy"},"type":"lvl2","url":"/syllabus#quiz-exam-policy","position":24},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Quiz/Exam Policy"},"content":"There are no quizzes or in-class exams in this course.","type":"content","url":"/syllabus#quiz-exam-policy","position":25},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Course Technology"},"type":"lvl2","url":"/syllabus#course-technology","position":26},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Course Technology"},"content":"Students should bring a laptop and charger to each class. You do not need a\nfancy machine -- any device with a keyboard, web browser and wifi should be sufficient\nto connect to our cloud-based compute platform which will be doing all the heavy lifting.\nIf you don’t have access to a laptop please request a device through our campus \n\nSTEP program.","type":"content","url":"/syllabus#course-technology","position":27},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Disability accommodations"},"type":"lvl2","url":"/syllabus#disability-accommodations","position":28},{"hierarchy":{"lvl1":"⚖ Syllabus","lvl2":"Disability accommodations"},"content":"Our course aspires towards \n\nuniversal design\nprinciples to minimize the need for common accommodations such as extra time on written exams or\nrecording devices for lectures (we have no timed exams and no lectures).\nWe are always keen to improve the design and provide additional accomdations to make this course accessible for all students.\n\nIf you need an accommodation for a disability, if you have information your wish to share with\nthe instructor about a medical emergency,\nor if you need special arrangements if the building needs to be evacuated, please inform the\ninstructor as soon as possible.\n\nIf you are not currently listed with DSP (the Disabled Students’ Program) and believe you might\nbenefit from their support, please apply online at \n\nhttps://​dsp​.berkeley​.edu.","type":"content","url":"/syllabus#disability-accommodations","position":29},{"hierarchy":{"lvl1":"Preface"},"type":"lvl1","url":"/preface","position":0},{"hierarchy":{"lvl1":"Preface"},"content":"Welcome to ESPM-157: Data Science in Global Change Ecology. This in-development course presents an opinionated approach to learning the tools of data science through the lens of global change problems.  Many of these opinions are anchored in deeply researched pedagogy and experience, some are not.\n\nAfter seven years of teaching in R, this version of the course is taught in Python.  The R community has benefitted tremendously in recent years from an close-knit, inclusive and welcoming community of instructors and software developers, including a notable committment to learning and pegagogy from the company formerly known as RStudio its team of open source developers of the “tidyverse” under Hadley Wickham.  Jenny Bryan (Statistics professor from UBC and a developer at RStudio) became an internet sensation for her ground-breaking open course, \n\nSTAT-545, which heavily influenced the early development of ESPM-157 and many similar courses in R, which among other things helped pioneer teaching GitHub ‘from day 1’ and led to the creation of \n\nhttps://​happygitwithr​.com.\n\nIt’s Abstractions All the Way Down, JD Long, Posit-Conf 2023.\n\nAdditional reading\n\nData Science Lifecycle in DS100 book, Learning Data Science By Sam Lau, Joey Gonzalez, and Deb Nolan.\n\nIntro in R for Data Science\n\nR4DS view:\n\nThe DS100 view of data cycle","type":"content","url":"/preface","position":1},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub"},"type":"lvl1","url":"/github-jupyter","position":0},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub"},"content":"GitHub & Jupyter are two core platforms in our course and across data science.  Over the course of this semester we will becoming very familiar with both, beginning with the most commonly used features and progressing to leveraging the more advanced corners of each.","type":"content","url":"/github-jupyter","position":1},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub","lvl2":"Cloud Platforms"},"type":"lvl2","url":"/github-jupyter#cloud-platforms","position":2},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub","lvl2":"Cloud Platforms"},"content":"Both GitHub and JupyterHub are cloud based platforms.  The term “cloud” is a fanciful way of saying that this software is running on “someone else’s computer,” usually (but not always), this means in a massive data center with rack upon rack of computers. Many of these data centers are owned by one of only a handful of ‘cloud providers’ -- such as Amazon Web Services (AWS), Google, Microsoft, or IBM, among others.  In both cases, we will access these platforms through a web browser.\n\nA key concept that takes some getting used to is thinking about where our data ‘lives’ when working on these platforms.  Remember that with each of these platforms, we are working on ‘someone else’s computer’.  The browser only gives us a window into that computer. Documents you create on JupyterHub don’t automatically exist on your laptop.  If your laptop completely died and you logged into JupyterHub on another platform, your work would still be there. However, if your partner logs into their JupyterHub, they are allocated their own separate computer.  To share your work with your partner, your instructors, or the larger world, we will be using GitHub. GitHub is running on yet another computer (actually many connected computers), and we must push our work from the JupyterHub machine to GitHub, or pull changes from GitHub into JupyterHub.  While many cloud-based services (like DropBox) automate some of these steps, data science tools such as git give us much more control of what and when we share.","type":"content","url":"/github-jupyter#cloud-platforms","position":3},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub","lvl2":"GitHub"},"type":"lvl2","url":"/github-jupyter#github","position":4},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub","lvl2":"GitHub"},"content":"GitHub is a commercial website platform acquired by Microsoft and widely used by software developers for collaborative work on both public and private code bases.  GitHub is built git, an open-source software for version control orginally created by Linus Torvalds, the creator of Linux. Many other open and proprietary platforms also use git as their foundation (notably GitLab and HuggingFace).  While many other open source version control utilities exist besides git, git is easily the most widely used today. GitHub is a primary mechanism for ‘publishing’ or distributing software source-code, notebooks and much else.  We will be using GitHub as a way of sharing code with your partner and submitting your assignments. While GitHub’s web interface is quite user friendly, the underlying technology of git has a reputation for being quite complex -- in fact based on precisely the same technological ideas as the “block-chain”, although it has existed long before anyone decided to apply those ideas to currency. Many instructors therefore avoid teaching GitHub and git in courses!\n\nCreate an acount on \n\nGitHub\n\nUse the GitHub Classroom magic link distributed in class to create a GitHub repository under the course’s GitHub Organization \n\nespm-157.\n\nContinue below.  As we progress, we will learn more about how to use git and GitHub -- how to recover earlier versions, create branches and pull requests, leverage automation in GitHub Actions and much else.","type":"content","url":"/github-jupyter#github","position":5},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub","lvl2":"JupyterHub"},"type":"lvl2","url":"/github-jupyter#jupyterhub","position":6},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub","lvl2":"JupyterHub"},"content":"The \n\nProject Jupyter is an open-source initiative created and led by our own \n\nFernando Perez and collaborators around the world. JupyterHub will be our home platform for the most of our work, where we will write and run code to perform tasks. Because we are running ‘on the cloud’ -- on someone else’s computer, this avoids the requirement of having to install all the necessary software used in this course on your laptop.  This also means that the computational heavy-lifting is all performed by the remote computer -- so we will not rely on students having powerful new laptops to complete computationally intensive tasks. This does not mean that it is necessary to use a cloud provider to do things we do. All the software we are using is free and open source and can be installed on your own computer.  Indeed, it is even possible for your computer to act like it’s own data center, talking to itself over precisely the same network protocols we would use to talk to a distant machine. In this way, JupyterHub ‘abstracts’ the computational environment away from the physical hardware, making it easy to deploy a workflow on machine of the appropriate size for the task at hand -- be it a tiny application or massive job for a supercomputing center.\n\nLog in to our JupyterHub, \n\nhttps://​nature​.datahub​.berkeley​.edu\n\nJupyterHub, JupyterLab, and Jupyter Notebooks -- the Jupyter ecosystem is a rich and changing landscape, which can often be confusing!  A JupyterHub (such as ours, \n\nhttps://​nature​.datahub​.berkeley​.edu, sometimes refered to at Berkeley as a DataHub) can serve many users with individual instances of the JupyterLab (a web-based \n\nIDE), which can in turn allow a user to work with many Jupyter Notebooks (and many other file formats and interfaces, including RStudio and the open-source Visual Studio Code Server).  Jupyter Notebooks are individual files (previously known as ipython notebooks, and still indicated with ipynb extension), a JSON-based serialization combining code, code outputs, and markdown text.  These notebooks are used throughout the data science community in industry and academia, and can be used not only in a JupyterLab IDE but in a wide variety of IDEs that have shamelessly copied it, including Google Colab, Microsoft VSCode, and Amazon SageMaker.","type":"content","url":"/github-jupyter#jupyterhub","position":7},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub","lvl2":"Initial setup"},"type":"lvl2","url":"/github-jupyter#initial-setup","position":8},{"hierarchy":{"lvl1":"Intro to GitHub & JupyterHub","lvl2":"Initial setup"},"content":"Authentication\n\nThere’s a few one-time setup steps we need to work with Git and GitHub from Jupyterhub.  First, in order to access private repositories, we need a way for our JupyterHub instance to authenticate our identity when trying to access our private stuff on GitHub, or when trying to share (write) stuff to our GitHub account. For security purposes, we will use a token-based authentication process to authenticate with GitHub.\n\nTry to clone our “authentication” repo, \n\nhttps://​github​.com​/espm​-157​-f24​/auth into your Jupyterhub using the Clone Repository button from the Git menu (left side).  (We will review this together in class).\n\nThen use the “File” menu on the left side to open the auth folder and then opn the Jupyter Notebook, github-login.ipynb.  Try running the cell in that notebook.\n\nTada! You have authenticated with GitHub on your Jupyterhub.  In future, you will only need to re-open this login notebook to authenticate, as it will remain on your JupyterHub.\n\nYou can now clone private repositories.  Note that before you can clone a new repository, you will need to navigate your file-pane out of the current auth/ repo and back to the home directory by clicking the folder icon.\n\nIntroduce yourself to git\n\nCommits in Git are ‘signed’ to indicate who made the changes.  The first time git is set up it needs to know the user’s name and email by running the following commands in a terminal.  Use the “launcher” menu in Jupyter to open a new terminal and run the following commands.git config --global user.name \"Your Name\"\ngit config --global user.email \"you@berkeley.edu\"\n\nSome users perfer to use pseudonyms. The email does not have to be valid, though using the same address as you registered with GitHub will let it display your commits as comming from you.  Now that git knows who you are, we are ready to start making our own commits.","type":"content","url":"/github-jupyter#initial-setup","position":9},{"hierarchy":{"lvl1":"Visualization Basics"},"type":"lvl1","url":"/visualization-basics","position":0},{"hierarchy":{"lvl1":"Visualization Basics"},"content":"Courses involving computer programming frequently start with control structures and data structures.  Extensive education research has underscored the value in starting with visualization. Visualization offers ‘early wins’, producing something that is compelling and often fun. Like many of our skills, we will continue to refine and extend our knowledge of data visualization throughout each stage of the course,  Mastery comes through repetition.\n\n","type":"content","url":"/visualization-basics","position":1},{"hierarchy":{"lvl1":"Visualization Basics","lvl2":"The Grammar of Graphics"},"type":"lvl2","url":"/visualization-basics#the-grammar-of-graphics","position":2},{"hierarchy":{"lvl1":"Visualization Basics","lvl2":"The Grammar of Graphics"},"content":"The seaborn Object Interface implements the \n\nGrammar of Graphics in python. This approach to data visualization provides a powerful abstraction around data visualization concepts. The theory has been around for a long time, but has become mainstream in data science through the work of Hadley Wickham and ggplot2 implementation in R.  Python libraries have only begun to adopt this concept more recently, but it can now be found in many python visualization libraries, including plotnine, altair, and seaborn’s object interface.[^1]\n\n[1]: Choosing packages is an important but tricky question. Many academic courses pretend to dismiss the issue, arguing that if students learn the fundamental principles, they will always be able to use the whatever package is fashionable at the time.  There is some truth to this, but ignores obvious realities in the learning experience.  The large, decentralized and rapidly moving landscape of python nevertheless makes this choice difficult, especially in comparison to the R language, where for the past decade at least, a single company with intense focus on pedagogy and predictable standards have created and promoted the tightly integrated and widely adoptied tidyverse.  Of those mentioned implementing a grammar of graphics: plotnine is nearly-identical translation of the ggplot2 syntax into python. It has only the core functionality of ggplot2 and a smaller user community.  altair provides python bindings to the popular vega library in javascript which also implementing the grammar of graphics, and can be particularly useful in generating dashboards where interactive javascript can enrich the design. seaborn is already a popular within the Python community, and it’s recently added object-oriented syntax provides a more ‘pythonic’ design than the aforementioned examples built around the syntax of other languages.\n\nThe fundemental principle of the grammar of graphics is that it expresses mappings from data to aesthetics of a graph (color, size, x/y position, etc). For instance, different colors are often used in graphs to indicate different groups:\n\nimport ibis\ndf = ibis.read_csv(\"../data/co2.csv\")\ndf.to_pandas()\n\nThe grammar of graphics always maps data (i.e. columns) to abstract aesthetics.  For instance, the categorical values in the “name” column can be mapped to indicate the color:\n\n# Modern\nimport seaborn.objects as so\n(\n    so.Plot(df, x = 'decimal_date', y='value', color=\"name\")\n    .add(so.Line())\n)\n\nMany early plotting engines such as matplotlib and even the original seaborn do not fully reflect these principles. Learning the grammar makes it much easier to make and modify complex charts, while also encouraging best-practices such as tidy data and semantic code.\nContrast this code with an alternative syntax that specifies the color manually:\n\n# Example of older verbose code\nimport seaborn as sns\ndf1 = df.pivot_wider()\nsns.lineplot(df1, x=\"decimal_date\", y = \"co2\", color=\"blue\")\nsns.lineplot(df1, x=\"decimal_date\", y = \"smooth\", color=\"red\")\n\nIn this syntax, x and y still refer to columns of the data, but color is asserted manually.  The relationship between color and data structure is not captured here.  If we wanted to use a different color scheme, we would have to manually update each assertion.  This is harder to generalize and more error-prone.  Note also that while this approach is somewhat more concise for a single x/y line, it quickly becomes more verbose, repeating many parts that do not need repeating in the previous approach.  We will avoid this older syntax.\n\nIn this example we can spot other advantages as well, such as default color theme informed by visualization research, and syntax that encourages the use of “tidy” data, in which columns represent variables and rows represent observations. This is sometimes called ‘long’ form, and more formally known as Cobb’s Third Normal Form. This format takes some getting used to, and frequently requires some transformation or ‘tidying’ to achieve, since many wild-caught data examples do not follow this best practice, especially older and smaller datasets.  We will have more to say about this later.\n\nAdditional Reading\n\nRead through the following tutorials on Seaborn\n\nObject Interface Introduction\n\nObject Interface Walk-through\n\nReferences\n\nSeaborn Object Interface","type":"content","url":"/visualization-basics#the-grammar-of-graphics","position":3},{"hierarchy":{"lvl1":"Accessing Real World Data"},"type":"lvl1","url":"/data-access","position":0},{"hierarchy":{"lvl1":"Accessing Real World Data"},"content":"Real world data is messy.  Most classes present students with data that has been cleaned up for ‘instructional purposes’, readily available and subset to small, easy-to-work-with sizes.  This leaves many students of data science or computer science rather ill-prepared in encountering real-world data, as the very first steps required before they can apply all their knowledge of processing, modeling and visualizing are precisely the steps we didn’t teach.  In this course, we work with real-world data.","type":"content","url":"/data-access","position":1},{"hierarchy":{"lvl1":"Accessing Real World Data","lvl2":"pandas and tabular data"},"type":"lvl2","url":"/data-access#pandas-and-tabular-data","position":2},{"hierarchy":{"lvl1":"Accessing Real World Data","lvl2":"pandas and tabular data"},"content":"","type":"content","url":"/data-access#pandas-and-tabular-data","position":3},{"hierarchy":{"lvl1":"Accessing Real World Data","lvl2":"earthdata"},"type":"lvl2","url":"/data-access#earthdata","position":4},{"hierarchy":{"lvl1":"Accessing Real World Data","lvl2":"earthdata"},"content":"","type":"content","url":"/data-access#earthdata","position":5},{"hierarchy":{"lvl1":"Accessing Real World Data","lvl2":"ibis"},"type":"lvl2","url":"/data-access#ibis","position":6},{"hierarchy":{"lvl1":"Accessing Real World Data","lvl2":"ibis"},"content":"","type":"content","url":"/data-access#ibis","position":7},{"hierarchy":{"lvl1":"Transforming Data"},"type":"lvl1","url":"/transform","position":0},{"hierarchy":{"lvl1":"Transforming Data"},"content":"","type":"content","url":"/transform","position":1},{"hierarchy":{"lvl1":"Transforming Data","lvl2":"Tidy Data"},"type":"lvl2","url":"/transform#tidy-data","position":2},{"hierarchy":{"lvl1":"Transforming Data","lvl2":"Tidy Data"},"content":"Analysis-ready data, or ‘tidy data’, is a central concept in data science.  The term was coined by Hadley Wickham to describe best practices long known in the relational database community, specifically, conventions around formatting tabular data known as \n\nCobb’s Third Normal Form. In this form, each row represents an observation and each column represents a variable. These conventions allow us to leverage powerful software and techniques for data analysis and visualization more easily, as many such tools are designed with tidy data structures in mind.  However, wild-caught data often does not follow these rules. Consider this example of NASA data on land ice mass:\n\nWickham (2014). Tidy data.  Journal of Statistical Software 14(10) DOI:10.18637/jss.v059.i10 \n\nhttps://​vita​.had​.co​.nz​/papers​/tidy​-data​.pdf\n\nimport pandas as pd\n\ndf = pd.read_csv(\"http://climate.nasa.gov/system/internal_resources/details/original/499_GRN_ANT_mass_changes.csv\",\n                skiprows=10, names=['date', 'greenland', 'antarctica'])\ndf\n\nThe column heading we have called ‘greenland’ is not a single variable (feature) being observed, but in fact a mashup of two different variables -- the ice mass (in gigatons) on the location of Greenland. (In the original data file it has the name Greenland mass (Gt), which more clearly indicates this mash-up but does not follow our naming conventions.  Likewise the column called ‘antarctica’ again measures land ice mass, but at a different location.  A “tidy” version of this data would use one column to indicate location (Greenland or Antarctica) and another column to indicate ice mass at that location.\n\nTo fix this, we need to pivot the data table into a “long” form, with a single column for ice mass and a single column for location.  While there are many ways to do this, we will rely on the ibis framework which provides one of the most consistent and powerful interfaces for tabular data operations.  We begin by loading the ibis library and establishing a “connection” to a backend engine.\n\nimport ibis\nfrom ibis import selectors as s\ncon = ibis.duckdb.connect()\n\nWe add our pandas table to the ibis connection using the method create_table.  This allows us to manipulate the table using ibis methods rather than pandas.  We will later see other ways to create ibis tables directly from a wide range of data other than pandas data.frames.\n\nWe then use the pivot_longer() method to transform our data.  We use the ibis.selectors methods to indicate which columns we are aggregating in the pivot.  Optionally, we indicate the name of the new column that will contain the previous column names (greenland, antarctica) and the name of the column that will contain the cell values (the ice mass).\n\nland_ice = (con\n .create_table(\"land_ice\", df, overwrite=True)\n .pivot_longer(s.any_of('greenland', 'antarctica'),\n               names_to= \"location\", values_to = \"ice_mass\")\n)\n\nland_ice.execute()\n\nNote the use of .execute() at the end of the ibis command to see the results.  ibis uses a sophisticated strategy called “lazy evaluation”, in which it does not actually run the code we provide until absolutely necessary.  While this feels cumbersome now, these small extra steps will pay big dividends for us when we move to big data sources that historically courses have needed to teach entirely different tools for.\n\nNow that are data is ‘tidy’, some operations are much easier. For instance, plotting two lines, with a legend, merely requires the single aesthetic mapping of color to the location column: color = \"location\"\n\nimport seaborn.objects as so\np = (so.Plot(land_ice, \n             x = \"date\", \n             y = \"ice_mass\", \n             color = \"location\")\n     .add(so.Line())\n    )\n\np\n\nWe can map this additional column to different aesthetics just as easily.  This illustrates the value of well-structured data and abstract grammar for operating on it.  For instance, having subplots (facets) based on this data:\n\np.facet(\"location\")","type":"content","url":"/transform#tidy-data","position":3},{"hierarchy":{"lvl1":"ibis Single Table Verbs"},"type":"lvl1","url":"/ibis-1","position":0},{"hierarchy":{"lvl1":"ibis Single Table Verbs"},"content":"We will be focusing on how to use the ibis package, a successor to the popular pandas package, for manipulating tabular data. We begin by importing the ibis package.   (We include two additional imports from the package which are commonly referred to using their short names, the table placeholder _ instead of ibis._, and the selectors methods as s instead of the verbose ibis.selectors.  We will see these in action later).","type":"content","url":"/ibis-1","position":1},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"Learning Goals"},"type":"lvl2","url":"/ibis-1#learning-goals","position":2},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"Learning Goals"},"content":"establish a connection with duckdb.connect()\n\nuse head() and excute() to preview large data\n\nuse select(), distinct(), filter() to explore data.","type":"content","url":"/ibis-1#learning-goals","position":3},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"Getting started"},"type":"lvl2","url":"/ibis-1#getting-started","position":4},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"Getting started"},"content":"To use ibis, we must also select a backend.  We will always be using the quite new and very powerful duckdb backend for all of our tasks.  We select a backend by creating a “connection”.  The details here are not important for us, we can treat this first block as “boilerplate” starting code.\n\nimport ibis\nfrom ibis import _\nimport ibis.selectors as s\n\ncon = ibis.duckdb.connect()\n\nWe are now ready to read in our data.  We will begin by reading the metrics table from the direct access link, as indicated in the URL below.  con.read_csv() is quite similar to the pandas.read_csv() we saw in module 1, though the optional arguments get some different names and are not quite as flexibile.  One important option for our purposes will be the how to indicate missing values.  In the past, we’ve seen negative values like -99 be used to indicate missing values.  That convention reflects limitations of early software, which had no natural concept of “missing”. More modern conventions indicating missing values as “NULL” or “NA”.  We indicate the data has chosen the latter:\n\nmetrics_url = \"https://huggingface.co/datasets/cboettig/ram_fisheries/resolve/main/v4.65/tsmetrics.csv\"\ntsmetrics = con.read_csv(metrics_url, nullstr=\"NA\")\n\n\n","type":"content","url":"/ibis-1#getting-started","position":5},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"Previewing data: head() and execute()"},"type":"lvl2","url":"/ibis-1#previewing-data-head-and-execute","position":6},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"Previewing data: head() and execute()"},"content":"Let’s take a look at our new table:\n\ntsmetrics\n\nThis doesn’t look like a pretty pandas table! Where are the values?  Actually, as we become more familiar with ibis we learn to appreciate the display choice here.  ibis is designed for working with very big data. An important part of this is something called lazy evaluation. Even downloading a very large fle might take a long time, and trying to load a large dataset into python all at once can exceed available RAM and crash the kernel.  Instead, ibis merely “peeks” at the data over the remote connection -- without even downloading it! It tells us the names of each column and the data type (e.g. string, or numeric, etc) that the read_csv method has ‘guessed’ for the data.  As we will see, this is often the most useful information anway.\n\nIf we we do want to see a few example rows, we can use the method head() on the table, tsmetrics.head(), to say we want only want to see the top of the data frame.  Optionally we can specify how many rows we want to preview, e.g. tsmetrics.head(10) to see 10 (the default is 5). Let’s try it:\n\ntsmetrics.head()\n\nThat’s not the top of 5 rows!  Once again, ibis is being lazy.  We see the same definition of the table as before, only this time it has a name r0, and we see a “plan of execution”, that ibis will return the first 5 rows Limit[r0, 5].  We can force it to execute this plan with execute() :\n\ntsmetrics.head().execute()\n\nAt last, we are starting to see what the data really looks like. Data tables can quickly become much to large to explore by simply trying to eyeball every row.  For instance, we notice the first column, tscategory, shows a few different possible categories for the various metrics in the database.  So, how many distinct categories are there?","type":"content","url":"/ibis-1#previewing-data-head-and-execute","position":7},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"select() and distinct()"},"type":"lvl2","url":"/ibis-1#select-and-distinct","position":8},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"select() and distinct()"},"content":"To answer this, we will introduce a few more methods of data table manipulation. select() selects one or more columns of a given table, while distinct() returns only distinct (unique) rows of the table.  Note that both of these methods share a common pattern -- they both apply to a table (not some piece of a table, like a row or column or cell), and they both return a new table as well that is some subset of the old table.  table in, table out.  This design is very intentional -- by having methods designed specificially to operate on tables and return tables, we can easily stack or chain these together, (also true of head() and execute()`.  So let’s try and see distinct categories:\n\n(tsmetrics\n .select(\"tscategory\")\n .distinct()\n .head(10)\n .execute()\n)\n \n\nNote that we have stacked these methods together with each step on it’s own line by wrapping the whole thing inside () parentheses.  This can make a long “chain” of commands easier to read.  While we have asked for no more that 10 values, we have gotten back only 8 -- so we now know there are only 8 categories.","type":"content","url":"/ibis-1#select-and-distinct","position":9},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"filter()"},"type":"lvl2","url":"/ibis-1#filter","position":10},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"filter()"},"content":"What are the different unit types within, say, the “CATCH or LANDINGS” category?  This requires a subset of rows (a filter), rather than a subset of columns (select()):\n\n(tsmetrics\n .filter(_.tscategory == \"CATCH or LANDINGS\")\n .distinct()\n .head(14)\n .execute()\n)\n\nThis syntax to subset rows (filter) is more complicated than columns (select) -- to find rows containing “CATCH or LANDINGS” we have to indicate which column to look for.","type":"content","url":"/ibis-1#filter","position":11},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl3":"column selection and .","lvl2":"filter()"},"type":"lvl3","url":"/ibis-1#column-selection-and","position":12},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl3":"column selection and .","lvl2":"filter()"},"content":"For python to know that we are looking for the column called “tscategory”, we use the column selection _.tscategory.  This is actually a shorthand for the pattern tsmetrics.tscategory -- the _ is a placeholder for “the current table” in our chain.  Extracting a single column with . is itself something of a shorthand, it is equivalent to using the selector [, as tsmetrics[\"category\"].  When a column name is also the name of a table method, we may need to fall back on the square bracket convention.  So why use a dot at all?  In addition to taking two less characters to write, the . method allows “tab completion” of the column name, which helps us avoid typos.  Note that our select() method recognizes either syntax, you can do: tsmetrics.select(_.tscategory).  This looks slightly more cryptic, but benefits from autocomplete and matches the sytnax of other functions.","type":"content","url":"/ibis-1#column-selection-and","position":13},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl3":"== not =","lvl2":"filter()"},"type":"lvl3","url":"/ibis-1#id-not","position":14},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl3":"== not =","lvl2":"filter()"},"content":"Another common mistake is to use a single = sign rather than == in filter.  Recall that = is used in variable assignment, a = 1 sets the value of a as 1.  Double-equals is a “boolean operator”, that tests if the statement is True or False:\n\na = 1\na == 1\n\n\nOther boolean operators include >, >=, != (not equal) and so forth. The important thing is to know that we can do boolean comparisons, this syntax is easy to look up.","type":"content","url":"/ibis-1#id-not","position":15},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"Next steps"},"type":"lvl2","url":"/ibis-1#next-steps","position":16},{"hierarchy":{"lvl1":"ibis Single Table Verbs","lvl2":"Next steps"},"content":"Explore the datasets in this collection using select(), distinct() and filter().  We will return to this list of Catch units after we become more aquainted with the remaining tables.\n\nThis syntax is harder than select() -- we can’t just filter for “CATCH or LANDINGS” without indicating which column we are looking in.  To signal that tscategory is a column name and not just a piece of text, we use the subsetting notation, _.tscategory.  This is merely a shorthand for the more verbose:","type":"content","url":"/ibis-1#next-steps","position":17},{"hierarchy":{"lvl1":"ibis Table Joins"},"type":"lvl1","url":"/ibis-2","position":0},{"hierarchy":{"lvl1":"ibis Table Joins"},"content":"","type":"content","url":"/ibis-2","position":1},{"hierarchy":{"lvl1":"ibis Table Joins","lvl2":"Learning Goals"},"type":"lvl2","url":"/ibis-2#learning-goals","position":2},{"hierarchy":{"lvl1":"ibis Table Joins","lvl2":"Learning Goals"},"content":"use join() to combine two tables on a key column\n\nimport ibis\nfrom ibis import _\nimport ibis.selectors as s\n\ncon = ibis.duckdb.connect()\n\nLast time we started getting comfortable with lazy evaluation (head() and execute()) in ibis, and began to learn how to select() (subset columns) and  filter() (subset rows), as well as looking at distinct values. Today we will continue to draw on these skills as we go deeper into the fisheries data in search of the evidence of the North Atlantic Cod collapse.  In the process, we shall pick up some new methods as well.\n\nAs before, let’s start with reading in data. Rather than focus on the metrics table, this time we will connect to several tables at the same time.  Note how we can reuse the base_url to avoid extra typing, but take care that we reading the right CSV file in each case! As before, we explicitly set the nullstr value as well to ensure missing value codes are correctly interpreted.\n\nbase_url = \"https://huggingface.co/datasets/cboettig/ram_fisheries/resolve/main/v4.65/\"\n\nstock = con.read_csv(base_url + \"stock.csv\", nullstr=\"NA\")\ntimeseries = con.read_csv(base_url + \"timeseries.csv\", nullstr=\"NA\")\nassessment = con.read_csv(base_url + \"assessment.csv\", nullstr=\"NA\")\n\n\n","type":"content","url":"/ibis-2#learning-goals","position":3},{"hierarchy":{"lvl1":"ibis Table Joins","lvl2":"Fish ‘stocks’"},"type":"lvl2","url":"/ibis-2#fish-stocks","position":4},{"hierarchy":{"lvl1":"ibis Table Joins","lvl2":"Fish ‘stocks’"},"content":"Like most real world data science problems, understanding these tables requires both a bit of background in fisheries science and a lot of splunking into the data.  For our purposes, one of the key things you should know is that fisheries are divided into “stocks”, which you can think of as a particular species of fish in a particular area of the ocean. Let’s use the stock table to explore this idea a bit more.  Let’s begin with a peek at the stock table:\n\nstock\n\nAh! commonname looks like a good place as any to go looking for Atlantic cod.  Of course if we knew (or looked up) the scientific name of the species, that might be even better -- after all, common names are not always as precise.  Let’s see what we can find:\n\n(stock\n .filter(_.commonname == \"Atlantic cod\")\n .select(_.stockid, _.scientificname, _.commonname, \n         _.areaid, _.region, _.primary_country, _.ISO3_code)\n .head()\n .execute()\n)\n\nLots of stocks of Atlantic cod!  Each row begins with a unique stockid.  A column that uniquely identifies each row in a given table is often referred to as the “primary key” for that table (and is often but not necessarily listed first).\nThe rows that follow give us some sense of what defines a “stock” as a species in an area: we see a few different identifiers for the species: commonname, scientificname.\nWe also see information abot the area the stock occurs in -- such as areaid, region, and primary country.  (For display purposes we selected only a subset of columns).While we have found the Cod, we haven’t yet found any data about the cod catch over time!  For that we will need to look in the timeseries data.  Let’s see how it is organized:\n\ntimeseries.head().execute()\n\nWe again have a column called stockid. While we no longer have columns such as commonname or scientificname to tell us what species each row in the timeseries is measuring, we now know that we can look up that information in the stock table using the stockid. Such a column is often called a “foreign key”, because it matches the primary key of a separate table.  (it appears the timeseries data has no ‘primary key’ of it’s own -- no column that has a unique value for each row.). Rather than have to switch back and forth between two tables, we can join the two tables on stockid:\n\n(stock\n .filter(_.commonname == \"Atlantic cod\")\n .join(timeseries, \"stockid\")\n .head()\n .select(_.stockid, _.scientificname, _.tsid, _.tsyear, \n         _.tsvalue, _.stocklong, _.stocklong_right) # subset of columns to keep display narrow\n .execute()\n)\n\n\nEffectively all this has done is take our timeseries table and for each stockid, add extra columns explaining what the stock table tells us about the stockid - species names, areas, and so on. The join has made our data is much wider than before -- we have all the columns from both tables.  (Note that both tables happened to have one column with the same name, stocklong.  A truly tidy database would not have done this -- we can easily see that this information belongs in the stock table.  Because our database cannot assume these are the same when we join, it has renamed the one on the “right” (from timeseries) as “stocklong_right” to distinguish them).  Because each stockid was repeated in the timeseries table, now all this other information is repeated too.  This is not as inefficient as it may sound, thanks to internal optimizations in the database.\n\nWhile it is clear even from this head() preview that we have the columns from both tables, what about the rows?  Our stock table was already filtered to a subset of rows containing only Cod stocks.  This join (technically called an “inner join”) has kept only those stockids, so we now have timeseries only about Cod!  In fact, we could have instead joined the full tables for all stock ids, and then applied the filter for commonname.","type":"content","url":"/ibis-2#fish-stocks","position":5},{"hierarchy":{"lvl1":"ibis Table Joins","lvl2":"Exercise"},"type":"lvl2","url":"/ibis-2#exercise","position":6},{"hierarchy":{"lvl1":"ibis Table Joins","lvl2":"Exercise"},"content":"Try further exploring this resulting table using select() and distinct() to get a better sense of what rows are here.  You will notice additional “*id” columns, like asssesid or areaid matching other tables in the data.  Explore filtering and joinging with these tables as well.","type":"content","url":"/ibis-2#exercise","position":7},{"hierarchy":{"lvl1":"ibis mutates and aggregates"},"type":"lvl1","url":"/ibis-3","position":0},{"hierarchy":{"lvl1":"ibis mutates and aggregates"},"content":"","type":"content","url":"/ibis-3","position":1},{"hierarchy":{"lvl1":"ibis mutates and aggregates","lvl2":"Learning Goals"},"type":"lvl2","url":"/ibis-3#learning-goals","position":2},{"hierarchy":{"lvl1":"ibis mutates and aggregates","lvl2":"Learning Goals"},"content":"use group_by() and aggregate() patterns to summarize data\n\nuse order_by() to arrange rows by one or more columns.\n\nimport ibis\nfrom ibis import _\nimport ibis.selectors as s\n\ncon = ibis.duckdb.connect()\n\nbase_url = \"https://huggingface.co/datasets/cboettig/ram_fisheries/resolve/main/v4.65/\"\n\nstock = con.read_csv(base_url + \"stock.csv\", nullstr=\"NA\")\ntimeseries = con.read_csv(base_url + \"timeseries.csv\", nullstr=\"NA\")\nassessment = con.read_csv(base_url + \"assessment.csv\", nullstr=\"NA\")\n\n\n","type":"content","url":"/ibis-3#learning-goals","position":3},{"hierarchy":{"lvl1":"ibis mutates and aggregates","lvl2":"Stock “assessments”"},"type":"lvl2","url":"/ibis-3#stock-assessments","position":4},{"hierarchy":{"lvl1":"ibis mutates and aggregates","lvl2":"Stock “assessments”"},"content":"The next thing we need to know is a stock assessment.\n\nassessment\n\nAre some stockids assessed multiple times? One intuitive idea is to filter() for a single stockid, and see if we get back multiple rows (multiple assessments).  Let’s take a look at “COD2J3KL”:\n\n(assessment\n .filter(_.stockid == \"COD2J3KL\")\n .select(_.assessid, _.assessorid, _.stockid,\n         _.daterecorded, _.assessyear) # pick a subset of columns to focus on\n .execute()\n)\n\nIndeed, it looks like their are four assessments of this stock, each conducted in different years and spanning different periods in time!  filter()ing for each possible stockid would be tedious though. These four assessments that correspond to this stockid\n\n(assessment\n .group_by(_.stockid)\n .agg(n=_.count())\n .execute()\n)\n\n","type":"content","url":"/ibis-3#stock-assessments","position":5},{"hierarchy":{"lvl1":"ibis mutates and aggregates","lvl2":"order_by()"},"type":"lvl2","url":"/ibis-3#order-by","position":6},{"hierarchy":{"lvl1":"ibis mutates and aggregates","lvl2":"order_by()"},"content":"Which stockids have the most assessments?  We can re-order the rows by different columns using the order_by(). (Changing the row order does not alter any individual row itself -- that would mess up the data.  Each row is moved as a unit).  By default, order is always increasing, smallest to largest, A to Z.  While that might be intuitive for dates or names, if we want to see which stocks have the most assessments, we need n to be in descending order.  We indicate this by appending the .desc() method to the column:\n\n(assessment\n .group_by(_.stockid)\n .agg(n=_.count())\n .order_by(_.n.desc())\n .execute()\n)","type":"content","url":"/ibis-3#order-by","position":7},{"hierarchy":{"lvl1":"ibis + seaborn.objects: Data Exploration"},"type":"lvl1","url":"/ibis-4","position":0},{"hierarchy":{"lvl1":"ibis + seaborn.objects: Data Exploration"},"content":"","type":"content","url":"/ibis-4","position":1},{"hierarchy":{"lvl1":"ibis + seaborn.objects: Data Exploration","lvl2":"Learning Goals"},"type":"lvl2","url":"/ibis-4#learning-goals","position":2},{"hierarchy":{"lvl1":"ibis + seaborn.objects: Data Exploration","lvl2":"Learning Goals"},"content":"We are now ready to start assembling the information we have learned from our initial exploration and bring together our skills with ibis and seaborn.objects.\n\nimport ibis\nfrom ibis import _\nimport ibis.selectors as s\nimport seaborn.objects as so \n\ncon = ibis.duckdb.connect()\n\nbase_url = \"https://huggingface.co/datasets/cboettig/ram_fisheries/resolve/main/v4.65/\"\nstock = con.read_csv(base_url + \"stock.csv\", nullstr=\"NA\")\ntimeseries = con.read_csv(base_url + \"timeseries.csv\", nullstr=\"NA\")\n\nLast time we reached the conclusion that we wanted to average across multiple assessments:\n\ncod_stocks = (\n  timeseries\n  .join(stock, \"stockid\")\n  .filter(_.tsid == \"TCbest-MT\")\n  .filter(_.commonname == \"Atlantic cod\")\n  .group_by(_.tsyear, _.stockid, _.primary_country, _.primary_FAOarea)\n  .agg(catch = _.tsvalue.mean())\n)\n\n\nThis is great, but even after aggregating the assessments, we have a lot of individual cod stocks in various locations:\n\n(\n    so.Plot(cod_stocks, \n        x = \"tsyear\",\n            y=\"catch\",\n            color = \"stockid\")\n    .add(so.Lines(linewidth=3))\n    .layout(size=(10, 6))\n)\n\nOur good friend, the COD2J3KL series shows up with it’s remarkable declines, but what’s going on with those highly variable but very large catches?  This will obviously impact our assessment of whether or not the species as a whole has collapsed.  This is too many stocks to easily explore, let’s try breaking this out by at the by country:\n\n(\n    so.Plot(cod_stocks, \n            x = \"tsyear\", \n            y=\"catch\", \n            color = \"stockid\",\n            group = \"primary_country\")\n    .add(so.Lines(linewidth=3))\n    .facet(\"primary_country\", wrap = 6)\n    .layout(size=(16, 10))\n)\n\nNote in the country-based graphs, several countries have multiple stocks.  Norway and Canada stand out for the largest harvests.  The ‘grammar of graphics’ in Seaborn objects makes it easy to quickly visually explore the data along different dimensions.  For instance, we can divide stocks by primary FAO Area instead of country with a single change:\n\n(\n    so.Plot(cod_stocks, \n            x = \"tsyear\", \n            y=\"catch\", \n            color = \"stockid\",\n            group = \"primary_FAOarea\")\n    .add(so.Lines(linewidth=3))\n    .facet(\"primary_FAOarea\")\n    .layout(size=(12, 8))\n)\n\nWe have visually grouped the data into the two parts of the globe where Atlantic Cod are found: “Western North-Atlantic” (FAO area 21) and “Eastern North-Atlantic” (area 27).  While we see declines in some Eastern stocks, the pattern in the West is much more dramatic.  If we want to consider the fate of Western Atlantic Cod as a whole, we can add up all these individual stocks to get a picture for the entire FAO Region.  We call the resulting table cod_fao because now it no longer reflects individual stocks, we have summed all the individual stocks up when we aggregated to the level of entire FAO regions.\n\ncod_fao = (cod_stocks\n   .group_by(_.tsyear, _.primary_FAOarea)\n   .agg(catch = _.catch.sum())\n)\n\nOnce again we can get a visual sense of the resulting aggrecations.  \n\n(\n    so.Plot(cod_fao, \n            x = \"tsyear\", \n            y=\"catch\")\n    .add(so.Lines(linewidth=3))\n    .facet(\"primary_FAOarea\")\n)","type":"content","url":"/ibis-4#learning-goals","position":3}]}